{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93da8c4",
   "metadata": {},
   "source": [
    "###### Models and Optimatization\n",
    "Models have parameters, hyper-parameters/ad hoc, and they all need to be fitted.\n",
    "\n",
    "Model fitting/Optimizaiton: find the best solution from a set of feasible solutions.\n",
    "\n",
    "How do we define the best solution?\n",
    "\n",
    "Loss function: $\\mathscr{L}$\n",
    "\n",
    "$w^* = min_w\\mathscr{L}(f(w,x),t)$ for $w = (w_1, w_2,...w_m)$\n",
    "\n",
    "Example: Regression $min_{x\\in\\mathscr{R^n}} \\sum_{i=1}^{m}(f(w,x_i)-t_i)^2$\n",
    "\n",
    "Quadratic Loss function: sum-of-squares differences between output and target\n",
    "\n",
    "Parameters: weights w\n",
    "\n",
    "Hyper-parameters: model choices in f()\n",
    "\n",
    "E.g., linear regression, no. of hidden layers and no. of nodes in each in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef3738",
   "metadata": {},
   "source": [
    "###### Loss functions\n",
    "- what do we want to minimize?\n",
    "- what do we want to do?\n",
    "- Supervised learning\n",
    "- minimize the difference between the output and the target\n",
    "- averaged over all the training data\n",
    "- 2 kinds of output: continuous (regression), and binary or discrete (classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2e2c6",
   "metadata": {},
   "source": [
    "###### Generalization\n",
    "- We want to minimize the error on data we don't know\n",
    "- We have training data\n",
    "- We want the algorithm to work well on other (related) data (e.g., from the same probability distribution)\n",
    "- In other words, we want the algorithm to generalize from the training data to all examples\n",
    "- That's what validation aims to do. If you don't have enough data: cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e07db",
   "metadata": {},
   "source": [
    "###### Loss functions\n",
    "- [Regression] Quadratic loss (sum-of-squares): $\\mathscr{L} = (f(w,x)-t)^2$\n",
    "- [Regression] Binary loss: $\\mathscr{L}$ = 0 if x = t, or 1 otherwise\n",
    "- [Classification] Hinge loss: $\\mathscr{L}$ = max(0,1 - yt) , support Vector Machine (actually a constrained optimization problem)\n",
    "- [Classification] Cross-entropy: $\\mathscr{L} = -tlogy - (1 - t)log(1 - y)$\n",
    "   - most commonly-used loss function for classification\n",
    "   - this is a comparison of probability distributions (c.f., Kullback-Liebler Divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed89b4e",
   "metadata": {},
   "source": [
    "###### Model Fitting\n",
    "- choose a model\n",
    "- select hyperparameters\n",
    "- fit the model to the data by optimizing the parameter values\n",
    "- evaluate the model on independent test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d125206",
   "metadata": {},
   "source": [
    "Gradient Descent \n",
    "- Stochastic Gradient Descent \n",
    "    - Momentum \n",
    "        - Adam (Momentum + Adgrad)\n",
    "    - Adagrad \n",
    "        - RMSprop \n",
    "        \n",
    "- Gradient Descent: thuật toán tối ưu lặp (iterative optimization algorithm) phổ biến trong Machine Learning và Deep Learning. \n",
    "\n",
    "https://viblo.asia/p/optimizer-hieu-sau-ve-cac-thuat-toan-toi-uu-gdsgdadam-Qbq5QQ9E5D8\n",
    "\n",
    "https://machinelearningcoban.com/2017/01/16/gradientdescent2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f99d8",
   "metadata": {},
   "source": [
    "###### Algorithm #1: Gradient Descent\n",
    "- compute derivatives of the loss functin with respect to each weight/parameter\n",
    "- usually end up in a local minimum\n",
    "- head downhill or uphill\n",
    "    1. where to start?\n",
    "    2. how do we compute the downhill direction?\n",
    "        - take a step downhill: $w^{(t)} = w_{(t-1)} - \\eta\\sum^n_{i=1}\\nabla\\mathscr{L}_i(w^{(t-1)})$\n",
    "        - $\\eta$ : learning rate or step size : pre-chosen parameter\n",
    "    3. how far to travel\n",
    "        - repeat the process lots of times\n",
    "    4. when to stop\n",
    "        - it will converge to a local optimum (eventually)\n",
    "    \n",
    "This is called batch optimization: compute one gradient for all of the training data at each iteration.\n",
    "    \n",
    "###### Derivatives and Approximate Derivatives\n",
    "- computers aren't very good at infinitessimals\n",
    "- finite differences\n",
    "\n",
    "###### Iterations\n",
    "The algorithms we will see are iterative:\n",
    "- pick a start point {x^{(0)}}\n",
    "- while not at a minimum x* (or at least, near one):\n",
    "   - work out a downhill direction $\\Delta x$\n",
    "   - head in that direction $x^{(1)},x^{(2)},...,x^{(k)},...$\n",
    "\n",
    "$x^{(k+1)} = x^{(k)} - \\alpha^{(k)}\\Delta x^{(k)}$\n",
    "\n",
    "###### Local Optima and local minimum\n",
    "- ML functions have lots of parameters, such as, a deep neural network might have thousands\n",
    "\n",
    "###### Regularization\n",
    "- add extra terms to optimization problems\n",
    "  + reduce overfitting\n",
    "  + make the learning easier or faster\n",
    "- usually something to stop the weights getting too big:\n",
    "  + $R(w) = \\sum_i w^2_i$\n",
    "  + $R(w) = \\sum_i |w_i|$\n",
    "  + Occam's Razor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54521e",
   "metadata": {},
   "source": [
    "###### K-Means\n",
    "- given a set of datapoints, cluster them\n",
    "- find k centres that are the means of datapoints close to them\n",
    "- iterate this process\n",
    "\n",
    "$\\mathscr{L} = \\sum^K_{k=1}\\sum^m_{i=1} a_{ij}||x_i - \\mu_j||^2$\n",
    "\n",
    "where $a_ij = 1 if x_i \\in c_j$, or 0 otherwise\n",
    "\n",
    "$\\frac{\\partial\\mathscr{L}}{\\partial\\mu_j} = \\frac{\\partial}{\\partial\\mu_j} \\sum^m_{i=1}a_{ij}(x^T_ix_i - 2x^T_i\\mu_j + \\mu^T_j\\mu_j)$\n",
    "\n",
    "$= 2\\sum_i a_{ij}(\\mu_j - x_i)$\n",
    "\n",
    "$\\Rightarrow \\mu_j = \\frac{\\sum_i x_i}{n_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428bf58",
   "metadata": {},
   "source": [
    "###### Algorithm #2: Stochastic Gradient Descent \n",
    "- It can help to update after every datapoint instead $w^{(t)} = w^{(t-1)} - \\eta\\nabla\\mathscr{L}_i(w^{(t-1)})$\n",
    "- somehow avoids some local minima\n",
    "- randomly shuffle the dataset after each round of updates\n",
    "- a common compromize is to use min-batches\n",
    "    - subset of the data used to estimate each gradient\n",
    "- a good idea to adapt the learning rate\n",
    "\n",
    "###### Algorithm #3: Adam (adaptive momentum estimation)\n",
    "- the most common optimizer\n",
    "- uses 2 extra features:\n",
    "    + momentum: update previous direction as well as current one\n",
    "    + adaptive stepsize: modify the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5787257",
   "metadata": {},
   "source": [
    "###### Fitting hyper-parameters\n",
    "- harder - no derivatives since most hyper-parameters are categorical or integer values\n",
    "- tends to be some form of grid search or search method\n",
    "- work on cross-validated data, since it comes expensive on data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61366a5",
   "metadata": {},
   "source": [
    "###### Optimization without Gradients\n",
    "- Many problems where there are no derivatives (e.g., categorical or integer variables)\n",
    "- Most algorithms to find solutions are then stochastic\n",
    "    + hill-climbing: basic method which changes the solution a bit and keep it if it's better\n",
    "    + simulated annealing: keep worse solutions with a probability that decreases over time.\n",
    "    + evolutionary algorithms: simulate evolution to find better solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf177d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
